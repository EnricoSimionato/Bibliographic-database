{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4ugBnQJt8dk",
    "outputId": "85f42e41-b3d2-4c1b-de49-56addeabdf0a"
   },
   "outputs": [],
   "source": [
    "# Downloading pyspark\n",
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE THE ENVIRONMENT, UPLOAD DATA, PREPROCESS DATA AND CREATE THE TABLES: Author, Paper, Affiliation, Book, Journal and Conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# With sparkSession we create a connection to our database\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, xxhash64, collect_list, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "\n",
    "# Creating an entry point to the PySpark Application\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Bibliography\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Parameters\n",
    "INPUT_FILE = \"bibliography.json\"\n",
    "OPTIONS = {'multiline': 'true', 'allowNumericLeadingZero': 'true', 'timestampFormat': \"yyyy-MM-dd'T'HH:mm:ss[.ZZZ'Z']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#AUTHOR DATAFRAME\n",
    "# Schema of the Author DataFrame\n",
    "schemaAut = StructType(\n",
    "    [StructField('authors', ArrayType(StructType([\n",
    "        StructField('_id', StringType(), True),\n",
    "        StructField('name', StringType(), True),\n",
    "        StructField('email', StringType(), True),\n",
    "        StructField('bio', StringType(), True),\n",
    "    ])), True)\n",
    "     ])\n",
    "\n",
    "# Importing data from the JSON file\n",
    "df_aut = spark.read.format('json').options(**OPTIONS).schema(schemaAut).json(INPUT_FILE)\n",
    "\n",
    "# Creating one row of the DataFrame for each author of the array field 'authors'\n",
    "df_aut = df_aut.select(explode(df_aut.authors))\n",
    "df_aut = df_aut.withColumnRenamed(\"col\", \"authors\")\n",
    "# Filtering out the authors who have an identifier set to null and projecting the DataFrame on the wanted columns\n",
    "df_aut = df_aut.filter(col(\"authors._id\") != \"null\").select(\"authors._id\", \"authors.name\", \"authors.email\",\n",
    "                                                            \"authors.bio\")\n",
    "# Renaming the column containing the author identifier\n",
    "df_aut = df_aut.withColumnRenamed(\"_id\", \"author_id\")\n",
    "# Dropping duplicate rows\n",
    "df_aut = df_aut.dropDuplicates([\"author_id\"])\n",
    "\n",
    "# Visualizing data\n",
    "df_aut.printSchema()\n",
    "df_aut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PAPER DATAFRAME WITHOUT PUBLICATION_ID\n",
    "# Schema of the Paper DataFrame without the publication_id\n",
    "schemaPaper = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('title', StringType(), True),\n",
    "     StructField('keywords', ArrayType(StringType()), True),\n",
    "     StructField('fos', ArrayType(StringType()), True),\n",
    "     StructField('references', ArrayType(StringType()), True),\n",
    "     StructField('page_start', IntegerType(), True),\n",
    "     StructField('page_end', IntegerType(), True),\n",
    "     StructField('lang', StringType(), True),\n",
    "     StructField('abstract', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True),\n",
    "     StructField('date', TimestampType(), True),\n",
    "     StructField('doi', StringType(), True),\n",
    "     StructField('url', ArrayType(StringType()), True)\n",
    "     ])\n",
    "\n",
    "# Importing data from the JSON file\n",
    "df_paper = spark.read.format('json').options(**OPTIONS).schema(schemaPaper).json(INPUT_FILE)\n",
    "# Renaming the column containing the paper identifier\n",
    "df_paper = df_paper.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "\n",
    "# Visualizing data\n",
    "df_paper.printSchema()\n",
    "df_paper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# AFFILIATION DATAFRAME\n",
    "# Schema of the Affiliation DataFrame\n",
    "schemaAffiliation = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('authors', ArrayType(StructType([\n",
    "         StructField('_id', StringType(), True),\n",
    "         StructField('org', StringType(), True)\n",
    "     ])), True),\n",
    "     ])\n",
    "\n",
    "# Importing data from the JSON file\n",
    "df_aff = spark.read.format('json').options(**OPTIONS).schema(schemaAffiliation).json(INPUT_FILE)\n",
    "\n",
    "# Renaming the column containing the paper identifier\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"paper_id\")\n",
    "\n",
    "# Creating one row of the DataFrame for each author of the array field 'authors' for each paper\n",
    "df_aff = df_aff.select(\"paper_id\", explode(df_aff.authors))\n",
    "df_aff = df_aff.withColumnRenamed(\"col\", \"authors\")\n",
    "# Filtering out the authors who have an identifier set to null and projecting the DataFrame on the wanted columns\n",
    "df_aff = df_aff.filter(col(\"authors._id\") != \"null\").filter(col(\"paper_id\") != \"null\").select(\"paper_id\", \"authors._id\",\n",
    "                                                                                              \"authors.org\")\n",
    "# Renaming the column containing the author identifier\n",
    "df_aff = df_aff.withColumnRenamed(\"_id\", \"author_id\")\n",
    "# Dropping duplicate rows\n",
    "df_aff = df_aff.dropDuplicates([\"author_id\", \"paper_id\"])\n",
    "# Renaming the column containing the name of the organization\n",
    "df_aff = df_aff.withColumnRenamed(\"org\", \"organization\")\n",
    "\n",
    "# Visualizing data\n",
    "df_aff.printSchema()\n",
    "df_aff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# JOURNAL DATAFRAME\n",
    "# Preprocessing of the journals for cleaning and merging them\n",
    "\n",
    "# Schema of the Journal DataFrame\n",
    "journal_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Importing data from the JSON file\n",
    "df_journals_to_filter = spark.read.format('json').options(**OPTIONS).schema(journal_schema_preprocessing).json(\n",
    "    INPUT_FILE)\n",
    "\n",
    "# Filtering the DataFrame keeping only papers published in journals that have the identifying attributes set to a meaningful value\n",
    "df_journals_to_filter = df_journals_to_filter.filter(col('publication_type') == 'Journal').filter(\n",
    "    col('issn') != 'null').filter(col('venue') != 'null').filter(col('issue') >= 0).filter(col('volume') >= 0)\n",
    "# Aggregating the rows using the fields which univocal identify the journals and collecting the 'publisher' attribute and the '_id' attribute in two lists\n",
    "df_journals_to_filter = df_journals_to_filter.groupBy('venue', 'volume', 'issue', 'issn').agg(\n",
    "    collect_list('publisher').alias('publishersArray'), collect_list('_id').alias('_id'))\n",
    "# Keeping only one element 'publisher' for each row since the publisher was added randomly\n",
    "df_journals_to_insert = df_journals_to_filter.withColumn('publisher',\n",
    "                                                         df_journals_to_filter['publishersArray'][0]).select('venue',\n",
    "                                                                                                             'volume',\n",
    "                                                                                                             'issue',\n",
    "                                                                                                             'publisher',\n",
    "                                                                                                             'issn',\n",
    "                                                                                                             '_id')\n",
    "# Here we almost have the final Journal DataFrame. For a detailer explanation refer to chapter of the report\n",
    "\n",
    "# Adding the new column which contains the publication_id created using the function xxhash64\n",
    "df_journals = df_journals_to_insert.withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# Adding the \"foreign key\" publication_id of the publication to the linked papers\n",
    "\n",
    "# Exploding the Journal DataFrame obtaining one row for each paper published in a journal\n",
    "exploded_journals = df_journals.select(explode('_id'), 'publication_id')\n",
    "\n",
    "# Joining the exploded DataFrame and the Paper DataFrame in order to add to each paper the identifier of the publication on which it is published\n",
    "df_papers_in_journals = exploded_journals.join(df_paper, exploded_journals.col == df_paper.paper_id, \"inner\")\n",
    "df_papers_in_journals = df_papers_in_journals.drop('col')\n",
    "# Dropping the identifier of the papers used only for creating the relationship between Journal and Paper\n",
    "df_journals = df_journals.drop(df_journals._id)\n",
    "\n",
    "# Visualizing data\n",
    "#print('Papers')\n",
    "#df_papers_in_journals.show(truncate = False)\n",
    "print('Schema of the journals')\n",
    "df_journals.printSchema()\n",
    "print('Journals')\n",
    "df_journals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# BOOK DATAFRAME\n",
    "# Preprocessing of the books for cleaning and merging them\n",
    "\n",
    "# Schema of the Book DataFrame\n",
    "book_schema_preprocessing = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('isbn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Importing data from the JSON file\n",
    "df_books_to_filter = spark.read.format('json').options(**OPTIONS).schema(book_schema_preprocessing).json(INPUT_FILE)\n",
    "\n",
    "# Filtering the DataFrame keeping only papers published in books that have the identifying attributes set to a meaningful value\n",
    "df_books_to_filter = df_books_to_filter.filter(col('publication_type') == 'Book').filter(col('isbn') != 'null').filter(\n",
    "    col('venue') != 'null')\n",
    "# Aggregating the rows using the fields which univocal identify the books and collecting the 'publisher' attribute and the '_id' attribute in two lists\n",
    "df_books_to_filter = df_books_to_filter.groupBy('isbn', 'venue').agg(collect_list('publisher').alias('publishersArray'),\n",
    "                                                                     collect_list('_id').alias('_id'))\n",
    "# Keeping only one element 'publisher' for each row since the publisher was added randomly\n",
    "df_books_to_insert = df_books_to_filter.withColumn('publisher', df_books_to_filter['publishersArray'][0]).select(\n",
    "    'venue',\n",
    "    'isbn',\n",
    "    'publisher',\n",
    "    '_id')\n",
    "# Here we almost have the final Book DataFrame. For a detailer explanation refer to chapter 13 of the report\n",
    "\n",
    "# Adding the new column which contains the publication_id created using the function xxhash64\n",
    "df_books = df_books_to_insert.withColumn('publication_id', xxhash64('isbn', 'venue'))\n",
    "\n",
    "# Adding the \"foreign key\" publication_id of the publication to the linked papers\n",
    "\n",
    "# Exploding the Book DataFrame obtaining one row for each paper published in a book\n",
    "exploded_books = df_books.select(explode('_id'), 'publication_id')\n",
    "\n",
    "# Joining the exploded DataFrame and the Paper DataFrame in order to add to each paper the identifier of the publication on which it is published\n",
    "df_papers_in_books = exploded_books.join(df_paper, exploded_books.col == df_paper.paper_id)\n",
    "df_papers_in_books = df_papers_in_books.drop('col')\n",
    "# Dropping the identifier of the papers used only for creating the relationship between Book and Paper\n",
    "df_books = df_books.drop(df_books._id)\n",
    "\n",
    "# Visualizing the data\n",
    "# print('Papers')\n",
    "# df_papers_in_books.show(truncate = False)\n",
    "print('Schema of the books')\n",
    "df_books.printSchema()\n",
    "print('Books')\n",
    "df_books.show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CONFERENCE DATAFRAME\n",
    "# Preprocessing of the conferences for cleaning and merging them\n",
    "\n",
    "# Schema of the Conference DataFrame\n",
    "schemaConf = StructType(\n",
    "    [StructField('_id', StringType(), True),\n",
    "     StructField('location', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('publication_type', StringType(), True)])\n",
    "\n",
    "# Importing data from the JSON file\n",
    "df_conferences_to_filter = spark.read.format('json').options(**OPTIONS).schema(schemaConf).json(INPUT_FILE)\n",
    "\n",
    "# Filtering the DataFrame keeping only papers presented conferences that have the identifying attributes set to a meaningful value\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .filter(col('venue') != 'null')\n",
    "# Aggregating the rows using the fields which univocal identify the conferences and collecting the 'location' attribute and the '_id' attribute in two lists\n",
    "df_conferences_to_filter = df_conferences_to_filter \\\n",
    "    .groupBy('venue') \\\n",
    "    .agg(collect_list('location').alias('locations_array'), collect_list('_id').alias('_id'))\n",
    "# Keeping only one element 'location' for each row since the location was added randomly\n",
    "df_conferences_to_insert = df_conferences_to_filter \\\n",
    "    .withColumn('location', df_conferences_to_filter['locations_array'][0]) \\\n",
    "    .select('venue', 'location', '_id')\n",
    "# Here we almost have the final Conference DataFrame. For a detailer explanation refer to chapter 13 of the report\n",
    "\n",
    "# Adding the new column which contains the publication_id created using the function xxhash64\n",
    "df_conferences = df_conferences_to_insert.withColumn('publication_id', xxhash64('venue'))\n",
    "\n",
    "# Adding the \"foreign key\" publication_id of the publication to the linked papers\n",
    "\n",
    "# Exploding the Conference DataFrame obtaining one row for each paper presented in a conference\n",
    "exploded_conferences = df_conferences.select(explode('_id'), 'publication_id')\n",
    "\n",
    "# Joining the exploded DataFrame and the Paper DataFrame in order to add to each paper the identifier of the publication on which it is published\n",
    "df_papers_in_conferences = exploded_conferences.join(df_paper, exploded_conferences.col == df_paper.paper_id)\n",
    "df_papers_in_conferences = df_papers_in_conferences.drop('col')\n",
    "# Dropping the identifier of the papers used only for creating the relationship between Conference and Paper\n",
    "df_conferences = df_conferences.drop(df_conferences._id)\n",
    "\n",
    "# Visualizing the data\n",
    "#print('Papers')\n",
    "#df_papers_in_conferences.show(truncate = False)\n",
    "print('Schema of the conferences')\n",
    "df_conferences.printSchema()\n",
    "print('Conferences')\n",
    "df_conferences.show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Merging the 3 dataframe, each one contains the papers published in a specific media\n",
    "df_papers = df_papers_in_books \\\n",
    "    .union(df_papers_in_journals) \\\n",
    "    .union(df_papers_in_conferences)\n",
    "\n",
    "# Visualizing the data\n",
    "print('Papers schema')\n",
    "df_papers.printSchema()\n",
    "print('Papers data')\n",
    "df_papers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Performing some queries for checking the result\n",
    "print('Papers published in books')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Book') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "print('Papers published in conferences')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show()\n",
    "\n",
    "print('Papers published in journals')\n",
    "df_papers \\\n",
    "    .filter(col('publication_type') == 'Journal') \\\n",
    "    .select('paper_id', 'title', 'publication_type', 'publication_id') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMANDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 1: Add a new row to the Paper DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "new_paper_file = 'single_paper.json'\n",
    "\n",
    "# loading the paper with the Paper-schema define before\n",
    "new_paper = spark.read.options(**OPTIONS).json(new_paper_file, schemaPaper) \\\n",
    "    .withColumnRenamed(\"_id\", \"paper_id\")\n",
    "\n",
    "# defining journal schema\n",
    "journal_schema = StructType(\n",
    "    [StructField('issn', StringType(), True),\n",
    "     StructField('publisher', StringType(), True),\n",
    "     StructField('venue', StringType(), True),\n",
    "     StructField('volume', IntegerType(), True),\n",
    "     StructField('issue', IntegerType(), True)])\n",
    "\n",
    "# loading journal\n",
    "journal = spark.read.options(**OPTIONS).json(new_paper_file, journal_schema) \\\n",
    "    .withColumn(\"publication_id\", xxhash64('venue', 'volume', 'issue', 'issn'))\n",
    "\n",
    "# extracting the journal id\n",
    "# head returns a sequence/list of Row objects. Since the DF contain only this row\n",
    "# a list of only one row will be returned and we extract it with [0].\n",
    "# we transform the Row to a dictionary and we extract the 'publication_id' column\n",
    "foreign_key = journal.head(1)[0].asDict()['publication_id']\n",
    "\n",
    "# adding the journal to the journals dataframe if not already present\n",
    "if df_journals.filter(col('publication_id') == foreign_key).count() == 0:\n",
    "    df_jounrals = df_journals.union(journal)\n",
    "\n",
    "# extracting the paper ID\n",
    "paper_id = new_paper.head(1)[0].asDict()['paper_id']\n",
    "\n",
    "# Inserting foreign_key at position 1 (0 for programmers) of the DF\n",
    "# * select all the columns of the DF\n",
    "new_paper = new_paper.select(lit(foreign_key).alias('publication_id'), '*')\n",
    "\n",
    "# checking if the paper already exists before adding\n",
    "if df_papers.filter(col('paper_id') == paper_id).collect() == []:\n",
    "    df_papers = df_papers.union(new_paper)\n",
    "\n",
    "# checking the paper exists in the DF\n",
    "df_papers.filter(col('paper_id') == paper_id).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 2: update one single row of a dataframe (similar for multiple rows)\n",
    "# The command modifies the DOI and URL of a paper with identifier equal to '53e997e4b7602d9701fdb48a'\n",
    "\n",
    "from pyspark.sql.functions import lit, array\n",
    "\n",
    "# For performing the required update operation, we firstly filter the dataframe keeping only the row to be modified. This is done filtering for a specific value 'paper_id'\n",
    "updated_df_papers = df_papers \\\n",
    "    .filter(col('paper_id') == '53e997e4b7602d9701fdb48a')\n",
    "# We add a bunch of columns, each one containing a single value we want to insert, with the name new_fieldName\n",
    "updated_df_papers = updated_df_papers \\\n",
    "    .withColumn('new_doi', lit('10.1007/11944577_37')) \\\n",
    "    .withColumn('new_url', array([lit('https://link.springer.com/chapter/10.1007/11944577_37')]))\n",
    "# Then we drop the old columns containing the previous values, and we rename the new columns with the name of the old ones\n",
    "updated_df_papers = updated_df_papers \\\n",
    "    .drop(col('doi')).drop(col('url')) \\\n",
    "    .withColumnRenamed('new_doi', 'doi') \\\n",
    "    .withColumnRenamed('new_url', 'url')\n",
    "\n",
    "# We make the union between the entire dataframe, without the row we want to modify, and the new entry, using the command union.\n",
    "updated_df_papers = df_papers.filter(col('paper_id') != '53e997e4b7602d9701fdb48a').union(updated_df_papers)\n",
    "\n",
    "# To check the number of the paper is the same as before\n",
    "print('The size of the entire initial database is ' + str(\n",
    "    df_papers.count()) + ', the size of the current database is ' + str(updated_df_papers.count()))\n",
    "updated_df_papers.filter(col('paper_id') == '53e997e4b7602d9701fdb48a').select('paper_id', 'title', 'doi', 'url').show(\n",
    "    truncate=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 3: remove an entire column\n",
    "\n",
    "# We drop the column `lang` from the dataframe `df_papers`, and we assign the result to the variable `df_papers_without_lang`\n",
    "df_papers_without_lang = df_papers \\\n",
    "    .drop('lang')\n",
    "\n",
    "# We print the schema of the dataframe `df_papers_without_lang` to see if there are differences regarding `df_papers` dataframe.\n",
    "df_papers_without_lang.printSchema()\n",
    "# We print the first row of the `df_papers_without_lang` dataframe to visualize an example of new data in the database\n",
    "df_papers_without_lang.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 4: delete a group of rows\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Filter the data frame of the papers maintaining only the ones that have been published after 1951 and save the new version in the dataframe\n",
    "df_papers = df_papers \\\n",
    "    .filter(year('date') > '1950')\n",
    "\n",
    "# Show the papers, with some associated attributes, ordered by date\n",
    "df_papers.select('title', 'publication_type', 'date') \\\n",
    "    .orderBy('date') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Command 5: create a new column with the length of the paper (number of total pages)\n",
    "\n",
    "# Create a new dataframe starting from 'df_papers' by keeping only papers that have consistent values in 'page_start' and 'page_end'\n",
    "df_papers_total_pages = df_papers \\\n",
    "    .filter((col('page_start') >= 0) & (col('page_end') >= 0) & (col('page_start') <= col('page_end')))\n",
    "\n",
    "# Create an additional column 'total_pages' computed as the difference between 'pages_end' and 'page_start'\n",
    "df_papers_total_pages = df_papers_total_pages.withColumn('total_pages', col('page_end') - col('page_start'))\n",
    "\n",
    "# Only some fields are displayed just for better reading\n",
    "df_papers_total_pages \\\n",
    "    .select(col('title'), col('page_start'), col('page_end'), col('total_pages')) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 1: WHERE, JOIN\n",
    "# Retrieve all papers published on a specific issue and volume of a Journal\n",
    "\n",
    "# setting the values to perform the query with\n",
    "venue, volume, issue = ('BMC Bioinformatics', '14', '1')\n",
    "\n",
    "# We first filter by the values asssigned to the variable defined here\n",
    "# Then we join the DFs on publication_id col, imposing the condition on 'publication_type' value\n",
    "# the join will be done only when the two condition match\n",
    "df_papers_q1 = df_journals \\\n",
    "    .filter((col('venue') == venue) &\n",
    "            (col('volume') == volume) &\n",
    "            (col('issue') == issue)) \\\n",
    "    .join(df_papers,\n",
    "          (df_journals['publication_id'] == df_papers['publication_id']) &\n",
    "          (df_papers['publication_type'] == 'Journal'))\n",
    "\n",
    "df_papers_q1 \\\n",
    "    .select(['paper_id', 'title']) \\\n",
    "    .show(truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 2: WHERE, LIMIT, LIKE\n",
    "# The query returns the papers written in the last twenty years that have 'artificial' as substring of one of their keywords. We require that these papers have the DOI set to a not null value.\n",
    "# The results are ordered ascending by the date and only 15 elements are printed.\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp\n",
    "\n",
    "df = df_papers.withColumn('current time', current_timestamp())  # Adds a new column containing the current timestamp\n",
    "df \\\n",
    "    .filter((((unix_timestamp('current time') - unix_timestamp(\n",
    "    'date')) / 3600 / 24 / 365) < 20) &  # The filter keeps only the papers whose attribute 'doi' is not null and written in the last 20 years\n",
    "            (col('doi').isNotNull())) \\\n",
    "    .select('paper_id',\n",
    "            # The select function keeps only the columns 'paper_id', 'title', 'date' and 'keyword' which is a single element contained in 'keywords', expanded by the function explode\n",
    "            'title',\n",
    "            'date',\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').like('%artificial%')) \\\n",
    "    .distinct() \\\n",
    "    .select('title',\n",
    "            'date',\n",
    "            'keyword') \\\n",
    "    .sort(col('date').asc()) \\\n",
    "    .limit(15) \\\n",
    "    .show(truncate=50)\n",
    "# After the first select, we keep only the rows of the dataframe that have 'artificial' as a substring of the attribute 'keyword'. Each row now is the couple composed by one paper and one of its keywords\n",
    "# Then the duplicates are eliminated.\n",
    "# Finally, we select the attribute we are interested in, we sort the rows with respect to the publication date and the output is limited to 15 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 3: WHERE, IN, Nested Query\n",
    "# Show the papers collected in a book that have `multiagent system` as keyword.\n",
    "\n",
    "# We query the `df_papers` dataframe, and we assign the result to `nested_query` variable.\n",
    "# We filter the dataframe searching for the rows that have `keyword` value equal to `multiagent system` string.\n",
    "# In particular, the function `isin` requires in input a list of strings, but if the list has dimension one, then, it is equivalent in inserting a string instead of a list.\n",
    "nested_query = df_papers \\\n",
    "    .select('title',  # We select only the columns `title`, `publication_type`, `paper_id`, `date`, and `keyword`.\n",
    "            'publication_type',\n",
    "            'paper_id',\n",
    "            'publication_id',\n",
    "            'date',\n",
    "            # We explode the `keywords` column because it is a set, so we can create copies of the selected row with each of them a single string from the set assigned to the new column `keyword`\n",
    "            explode('keywords').alias('keyword')) \\\n",
    "    .filter(col('keyword').isin('multiagent system')) \\\n",
    "    .drop('keyword')\n",
    "\n",
    "# We query the `df_conferences` dataframe performing a `join` operation over the column `publication_id` with the dataframe `nested_query`.\n",
    "# When inside the `join` function we insert only the name of the column, then the function searches in both the dataframe to join if there is a header equal to the input string.\n",
    "# Then, we filter the joint dataframe selecting only the rows that have `publication_type` value equal to `Conference` because we are interested in papers that were published in conferences.\n",
    "# We order by `date` the dataframe, and we select only the `title` and `venue` columns to show.\n",
    "# Finally, we show the first five rows of the dataframe, and we format the visualization of the result thanks to the `truncate` parameter.\n",
    "df_conferences \\\n",
    "    .join(nested_query, 'publication_id') \\\n",
    "    .filter(col('publication_type') == 'Conference') \\\n",
    "    .orderBy('date', ascending=False) \\\n",
    "    .select(col('title').alias('paper title'),  # `alias` function renames a column with the string in input.\n",
    "            col('venue').alias('conference venue')) \\\n",
    "    .show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 4: GROUP BY, JOIN, AS\n",
    "# Retrieve the most prolific organizations regarding the conferences\n",
    "\n",
    "from pyspark.sql.functions import collect_set, size\n",
    "\n",
    "# We first perform a join between the dataframe of the affiliations and the dataframe of the papers base on the 'paper_id'\n",
    "# Then we drop one of the 'paper_id' columns present in the joined table, because in the select we need to extract the 'paper_id' so we have to eliminate ambiguity\n",
    "# Afterwards we filter the created dataframe, getting only elements that have an 'organization' that is not null or an empty string and elements that have as publication_type the type 'Conference'\n",
    "# Finally we group by the 'organization' and create for each organization a set that contains all the associated papers, identified by their id, then we filter the ones that have the set size > 10\n",
    "\n",
    "df_aff \\\n",
    "    .join(df_papers, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_papers.paper_id) \\\n",
    "    .select('paper_id',\n",
    "            'organization',\n",
    "            'publication_type') \\\n",
    "    .filter((col('organization').isNotNull()) &\n",
    "            (col('organization') != \"\") &\n",
    "            (col('publication_type') == \"Conference\")) \\\n",
    "    .groupBy('organization') \\\n",
    "    .agg(collect_set('paper_id').alias('papers')) \\\n",
    "    .filter(size(col('papers')) > 10) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 5: WHERE, GROUP BY\n",
    "# Retrieve some statistics about papers\n",
    "# In this query we use the DataFrame obtained by running the command 5.\n",
    "\n",
    "from pyspark.sql.functions import sum, min, max, avg, format_number, variance\n",
    "\n",
    "# Retrieve some statistics about papers published from the year 2015 on.\n",
    "# We group the articles by publication year using the year() function to extract the year from the date.\n",
    "# Then some aggregate functions are performed to obtain insights like:\n",
    "# the total number of papers published using count(),\n",
    "# the total number of pages written using sum(),\n",
    "# the minimum and the maximum number of pages in an article using min() and max(),\n",
    "# the mean of pages written per article and the variance using avg() and variance().\n",
    "# To round the fractional numbers we use format_number().\n",
    "# Years are shown in decreasing order.\n",
    "df_papers_total_pages \\\n",
    "    .filter(year(col('date')) >= 2015) \\\n",
    "    .groupBy(year(col('date')).alias('year')) \\\n",
    "    .agg(count('paper_id').alias('total_papers'),\n",
    "         sum('total_pages').alias('total_pages'),\n",
    "         min('total_pages').alias('min_pages'),\n",
    "         max('total_pages').alias('max_pages'),\n",
    "         format_number(avg('total_pages'), 2).alias('avg_pages'),\n",
    "         format_number(variance('total_pages'), 2).alias('var_pages')) \\\n",
    "    .sort(col('year').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 6: GROUP BY, HAVING, AS\n",
    "# Papers that are referenced the most and have at least 30 references\n",
    "\n",
    "# Firsly we explode the array field to have alll the references in rows\n",
    "# then we group by the reference so thaw we can perform a count operation on the papers\n",
    "# we filter by the value of the aggregation operation count\n",
    "# finally we do a join of the transformed DF with the original one so that we can\n",
    "# extract some meaningful information on the papers\n",
    "\n",
    "df_papers \\\n",
    "    .select('paper_id',\n",
    "            'title',\n",
    "            explode(col('references')).alias('reference')) \\\n",
    "    .groupBy('reference') \\\n",
    "    .agg(count('paper_id').alias('references_count')) \\\n",
    "    .filter(col('references_count') > 30) \\\n",
    "    .join(df_papers, col('reference') == df_papers.paper_id) \\\n",
    "    .sort(col('references_count').desc()) \\\n",
    "    .select(['title', 'references_count']) \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 7: WHERE, GROUP BY, HAVING, AS\n",
    "# The query returns the association between fields of study and keywords which are more present in the papers written after the year 2000 and how many times they appear together.\n",
    "\n",
    "#Explanation of the query\n",
    "# The first filter function keeps the papers that verify the following conditions: not null 'doi', year of publication bigger than 2000, at least one fos and one keyword\n",
    "# Then we explode 'keywords' and 'fos' arrays obtaining all the couples fos-keyword which appear together inside some paper\n",
    "# We group by the couples fos-keyword, and we count how many times the couple appears\n",
    "# We filter keeping only the fields of study and keywords coupled more than 100 times inside the database\n",
    "# Finally, we order the DataFrame by the number of occurrences of the couple ('couple count') and we limit the printed results\n",
    "\n",
    "from pyspark.sql.functions import year, col, size\n",
    "\n",
    "df = df_papers \\\n",
    "    .filter(\n",
    "    (col('doi').isNotNull()) &\n",
    "    (year(col('date')) >= 2000) &\n",
    "    (size(col('fos')) > 0) &\n",
    "    (size(col('keywords')) > 0)) \\\n",
    "    .select('fos', explode('keywords').alias('keyword')) \\\n",
    "    .select('keyword', explode('fos').alias('fos')) \\\n",
    "    .groupby('fos', 'keyword') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'couple count') \\\n",
    "    .filter(col('couple count') > 100) \\\n",
    "    .sort(col('couple count').desc()) \\\n",
    "    .limit(15) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 8: WHERE, Nested Query (i.e., 2-step Queries), GROUP BY\n",
    "# Retrieve the organizations associated with an author name for each field of study\n",
    "\n",
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "author_name = 'Hao Wang'  # We are searching for all the authors named 'Hao Wang.'\n",
    "\n",
    "# We query the `df_aut` dataframe by filtering out the authors that have name equal to the one defined by the `author_name` parameter.\n",
    "# From the result, we select only the column `author_id,` we collect all the values in a list, and we assign it to the `sub_nested_query` variable.\n",
    "sub_nested_query = df_aut \\\n",
    "    .filter(col('name') == author_name) \\\n",
    "    .select('author_id') \\\n",
    "    .rdd.flatMap(lambda x: x) \\\n",
    "    .collect()\n",
    "\n",
    "# We query the `df_aff` dataframe by filtering the rows that have `author_id` value inside the list `sub_nested_query.`\n",
    "# We also filter the dataframe from all the rows that have `organization` value equal to the string 'null.'\n",
    "# The result is assigned to the `nested_query` variable.\n",
    "nested_query = df_aff \\\n",
    "    .filter(col('author_id').isin(sub_nested_query)) \\\n",
    "    .filter(col('organization') != 'null')\n",
    "\n",
    "# Finally, we query the `df_papers` dataframe by firstly joining it over `paper_id` with the `nested_query` dataframe.\n",
    "# We explode the `fos` column because we want to extract its values to query them one by one, and then we assign its values to the column `field_of_study`.\n",
    "# We select from the joint dataframe the columns `paper_id`, `organization`, and `field_of_study`.\n",
    "# After selecting columns from the joint dataframe, we group the rows by `field_of_study`, and we aggregate the values `organization` into a set by assigning it to the column named `organization`.\n",
    "# Finally, we alphabetically order the column `field_of_study`, and we show the first fourteen rows of the result.\n",
    "df_papers \\\n",
    "    .join(nested_query, 'paper_id') \\\n",
    "    .select('paper_id',\n",
    "            'organization',\n",
    "            explode('fos').alias('field_of_study')) \\\n",
    "    .groupBy('field_of_study') \\\n",
    "    .agg(collect_set('organization').alias('organization')) \\\n",
    "    .orderBy('field_of_study') \\\n",
    "    .show(14, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 9: WHERE, GROUP BY, HAVING, 1 JOIN\n",
    "# Retrieve the most prolific publishers\n",
    "\n",
    "from pyspark.sql.functions import collect_set, concat, size\n",
    "\n",
    "# We select from the dataframe that contains the journals the elements which have at least 10 volumes\n",
    "# Then we join the journals dataframe and the books dataframe on the 'publisher' and drop one of the created columns 'publisher' to avoid duplicates\n",
    "# In the query we also renamed the columns that represent the venue of the Book and of the Journal, to have more clarity in the result and to avoid ambiguity on the columns\n",
    "# We select the relative venues and the publisher and drop duplicated elements, if there are, then we group by the 'publisher' and collect in two different sets the venueBooks and the venueJournals\n",
    "# Finally we concatenate the elements of the two sets creating a new column, and we filter on this final sets, keeping only the 'publishers' that have more than '500' associated publications\n",
    "# So we show the most prolific publishers truncating the show at '50' so the column representation is limited\n",
    "\n",
    "df_journals \\\n",
    "    .withColumnRenamed('venue', 'venueJournals') \\\n",
    "    .filter((col('volume')) > 10) \\\n",
    "    .join(df_books, df_books.publisher == df_journals.publisher, 'inner') \\\n",
    "    .drop(df_journals.publisher) \\\n",
    "    .withColumnRenamed('venue', 'venueBooks') \\\n",
    "    .select('venueBooks',\n",
    "            'venueJournals',\n",
    "            'publisher') \\\n",
    "    .dropDuplicates(['venueBooks',\n",
    "                     'venueJournals',\n",
    "                     'publisher']) \\\n",
    "    .groupBy('publisher') \\\n",
    "    .agg(collect_set('venueBooks').alias('books'),\n",
    "         collect_set('venueJournals').alias('journals')) \\\n",
    "    .withColumn('total_publications_per_publisher', concat('books', 'journals')) \\\n",
    "    .filter(size('total_publications_per_publisher') > '500') \\\n",
    "    .select('publisher',\n",
    "            'total_publications_per_publisher') \\\n",
    "    .show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Query 10: WHERE, GROUP BY, HAVING, 2 JOINs\n",
    "# Retrieve authors who worked for at least 3 different organizations and have published at least 3 papers with at least 5 fos and 5 references each\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "# Firstly, filter to keep only papers with at least 5 fos and 5 references.\n",
    "# Then join the paper table with the affiliation dataframe on 'paper_id' and then join it with the author dataframe on 'author_id', dropping the columns with ambiguous name.\n",
    "# Group on author_id to get all the aggregate information for each author.\n",
    "# To count the different organizations an author has worked for we use approx_count_distinct().\n",
    "# Filter again keeping authors that have worked for at least 3 organizations and published at least 3 papers.\n",
    "# For consistency, we check that only one name is associated with the grouped 'author_id'.\n",
    "# In the select part we explode 'name' field to obtain a single string instead of an array.\n",
    "# The result is decreasingly ordered by the number of papers and then the number of organizations.\n",
    "\n",
    "df_papers \\\n",
    "    .filter((size(col('fos')) >= 5) &\n",
    "            (size(col('references')) >= 5)) \\\n",
    "    .join(df_aff, df_papers.paper_id == df_aff.paper_id, 'inner') \\\n",
    "    .drop(df_aff.paper_id) \\\n",
    "    .join(df_aut, df_aff.author_id == df_aut.author_id, 'inner') \\\n",
    "    .drop(df_aff.author_id) \\\n",
    "    .groupBy('author_id') \\\n",
    "    .agg(count('paper_id').alias('papers_count'),\n",
    "         approx_count_distinct('organization').alias('organizations_count'),\n",
    "         collect_set('name').alias('name')) \\\n",
    "    .filter((size('name') == 1) &\n",
    "            (col('papers_count') >= 3) &\n",
    "            (col('organizations_count') >= 3)) \\\n",
    "    .orderBy(col('papers_count').desc(),\n",
    "             col('organizations_count').desc()) \\\n",
    "    .select(explode('name').alias('name'),\n",
    "            'papers_count',\n",
    "            'organizations_count') \\\n",
    "    .show(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
